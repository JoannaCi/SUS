{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemy uczące się - Zad. dom. 1: Minimalizacja ryzyka empirycznego\n",
    "\n",
    "### Autor rozwiązania: Joanna Cicha, 147963\n",
    "\n",
    "Celem zadania jest zaimplementowanie własnego drzewa decyzyjnego wykorzystującego idee minimalizacji ryzyka empirycznego. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twoja implementacja\n",
    "\n",
    "Twoim celem jest uzupełnić poniższą klasę `TreeNode` tak by po wywołaniu `TreeNode.fit` tworzone było drzewo decyzyjne minimalizujące ryzyko empiryczne. Drzewo powinno wspierać problem klasyfikacji wieloklasowej (jak w przykładzie poniżej). Zaimplementowany algorytm nie musi (ale może) być analogiczny do zaprezentowanego na zajęciach algorytmu dla klasyfikacji. Wszelkie przejawy inwencji twórczej wskazane. Pozostaw komenatrze w kodzie, które wyjaśniają Twoje rozwiązanie.\n",
    "\n",
    "Schemat oceniania:\n",
    "- wynik na ukrytym zbiorze testowym (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u 1 +20%,\n",
    "- wynik na ukrytym zbiorze testowym (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u 2 +40%,\n",
    "- wynik na ukrytym zbiorze testowym (automatyczna ewaluacja) celność klasyfikacji >= bardziej zaawansowanego baseline'u 3 +40%.\n",
    "\n",
    "Niedozwolone jest korzystanie z zewnętrznych bibliotek do tworzenia drzewa decyzyjnego (np. scikit-learn). \n",
    "Możesz jedynie korzystać z biblioteki numpy.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod), o ile będzie się w nim na koniec znajdowała kompletna implementacja klasy `TreeNode` w jednej komórce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate entropy of a probability distribution\n",
    "def H(p):\n",
    "    if p in (0, 1):\n",
    "        return 0.\n",
    "    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "# Function to calculate entropy of a dataset\n",
    "def entropy(array, n):\n",
    "    counts = np.unique(array, return_counts=True)[1]\n",
    "    s = np.sum(counts)\n",
    "    e = 0\n",
    "    for count in counts:\n",
    "        e += H(count/ s)\n",
    "    return len(array) / n * e\n",
    "\n",
    "# Function to calculate Gini impurity of a dataset\n",
    "def gini_impurity(array, n):\n",
    "    counts = np.unique(array, return_counts=True)[1]\n",
    "    s = np.sum(counts)\n",
    "    impurity = 1 - np.sum((counts / s) ** 2)\n",
    "    return len(array) / n * impurity\n",
    "\n",
    "# Function to calculate information gain based on criterion (Gini or entropy)\n",
    "def info_gain(zbior_1, zbior_2, criterion='gini'):\n",
    "    H_func = gini_impurity if criterion == 'gini' else entropy # Gini gives better accuracy result than entropy\n",
    "    if criterion == 'gini':\n",
    "        H_func = gini_impurity\n",
    "    else:\n",
    "        H_func = entropy\n",
    "    \n",
    "    # Calculate overall, left, and right impurity\n",
    "    n = len(zbior_1) + len(zbior_2)\n",
    "    h = H_func(np.hstack([zbior_1, zbior_2]), n)\n",
    "    h1 = H_func(zbior_1, n)\n",
    "    h2 = H_func(zbior_2, n)\n",
    "\n",
    "    # Calculate and return information gain\n",
    "    h = h - (len(zbior_1) / n * h1 + len(zbior_2) / n * h2)\n",
    "    return h\n",
    "\n",
    "# Function to find unique splitting points for a feature\n",
    "def get_splits(data, labels):\n",
    "    indices = np.argsort(data)\n",
    "    previous_cls = None\n",
    "    values = []\n",
    "    for value, cls in zip(data[indices], labels[indices]):\n",
    "        if previous_cls is None:\n",
    "            previous_cls = cls\n",
    "            continue\n",
    "        else:\n",
    "            if cls != previous_cls:\n",
    "                values.append(value)\n",
    "    values = np.unique(values)\n",
    "    return values\n",
    "\n",
    "# Function to create a boolean mask for splitting a dataset\n",
    "def split_mask(data, threshold):\n",
    "    return data < threshold\n",
    "\n",
    "# Function to find the best split for a dataset based on the specified criterion\n",
    "def find_best_split(data, labels, criterion='gini'):\n",
    "    igs = []\n",
    "    for col in range(data.shape[1]):\n",
    "        feature = data[:, col]\n",
    "        splits = get_splits(feature, labels)\n",
    "        for s in splits:\n",
    "            mask = split_mask(feature, s)\n",
    "            ig = info_gain(feature[mask], feature[~mask])\n",
    "            igs.append((ig, col, s))\n",
    "    if len(igs) < 1:\n",
    "        return (0.0, None, None)\n",
    "    return sorted(igs, reverse=True)[0]\n",
    "\n",
    "# Definition of the TreeNode class for the decision tree\n",
    "class TreeNode(object):\n",
    "    def __init__(self, depth=0, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.left = None # Typ: Node, wierzchołek znajdujący się po lewej stornie\n",
    "        self.right = None # Typ: Node, wierzchołek znajdujący się po prawej stornie\n",
    "        self.feature = None\n",
    "        self.split = None\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\t\n",
    "    def __format__(self, format_spec):\n",
    "        return str(self)\n",
    "    \n",
    "    # Recursive function to fit the decision tree to the training data\t\n",
    "    def fit(self, data, target, max_depth = None):\n",
    "        \"\"\"\n",
    "        Argumenty:\n",
    "        data -- numpy.ndarray, macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "        target -- numpy.ndarray, wektor klas o długości n, gdzie n to liczba przykładów\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find the best split for the current node\n",
    "        ig, feature, split = find_best_split(data, target)\n",
    "        if ig > 0. and len(data) >= self.min_samples_split and self.depth < self.max_depth:\n",
    "            self.feature = feature\n",
    "            self.split = split\n",
    "\n",
    "            # Create masks for left and right branches based on the split\n",
    "            mask = split_mask(data[:, feature], split)\n",
    "            X_train_left, y_train_left = data[mask], target[mask]\n",
    "            X_train_right, y_train_right = data[~mask], target[~mask]\n",
    "\n",
    "            # Recursively fit the left and right branches\n",
    "            self.left = TreeNode(depth=self.depth+1)\n",
    "            self.right = TreeNode(depth=self.depth+1)\n",
    "            self.left.fit(X_train_left, y_train_left, max_depth=max_depth)\n",
    "            self.right.fit(X_train_right, y_train_right, max_depth=max_depth)\n",
    "        else:\n",
    "            # If conditions are not met, assign the most common class to the leaf node\n",
    "            values, counts = np.unique(target, return_counts=True)\n",
    "            self.cls = values[np.argmax(counts)]\n",
    "\t\n",
    "    # Function to predict classes for new data points\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Argumenty:\n",
    "        data -- numpy.ndarray, macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "\n",
    "        Wartość zwracana:\n",
    "        numpy.ndarray, wektor przewidzoanych klas o długości n, gdzie n to liczba przykładów\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(data.shape[0])\n",
    "\n",
    "        # Check if the node is a leaf\n",
    "        if self.left is not None:\n",
    "            feature = self.feature\n",
    "            split = self.split\n",
    "            mask = split_mask(data[:, feature], split)\n",
    "\n",
    "            # Recursively predict for left and right branches\n",
    "            left, right = data[mask], data[~mask]\n",
    "            y_pred_left = self.left.predict(left)\n",
    "            y_pred_right = self.right.predict(right)\n",
    "\n",
    "            # Combine predictions for left and right branches\n",
    "            y_pred[mask] = y_pred_left\n",
    "            y_pred[~mask] = y_pred_right\n",
    "        else:\n",
    "            # If the node is a leaf, assign the class of the leaf to all instances\n",
    "            y_pred[:] = self.cls\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład trenowanie i testowania drzewa\n",
    " \n",
    "Później znajduje się przykład trenowania i testowania drzewa na zbiorze danych `iris`, który zawierający 150 próbek irysów, z czego każda próbka zawiera 4 atrybuty: długość i szerokość płatków oraz długość i szerokość działki kielicha. Każda próbka należy do jednej z trzech klas: `setosa`, `versicolor` lub `virginica`, które są zakodowane jak int.\n",
    "\n",
    "Możesz go wykorzystać do testowania swojej implementacji. Możesz też zaimplementować własne testy lub użyć innych zbiorów danych, np. innych [zbiorów danych z scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#toy-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
    "\n",
    "tree_model = TreeNode()\n",
    "tree_model.fit(X_train, y_train, max_depth = 3)\n",
    "y_pred_train = tree_model.predict(X_train)\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional test on breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Breast Cancer test set: 0.8138297872340425\n"
     ]
    }
   ],
   "source": [
    "tree_model_cancer = TreeNode(max_depth=5, min_samples_split=5, min_samples_leaf=2)\n",
    "tree_model_cancer.fit(X_train_cancer, y_train_cancer)\n",
    "y_pred_cancer = tree_model_cancer.predict(X_test_cancer)\n",
    "print(\"Accuracy on Breast Cancer test set:\", accuracy_score(y_test_cancer, y_pred_cancer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional test on wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on the Wine Quality dataset: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Use a specific attribute as the target variable (e.g., flavanoids)\n",
    "y = X[:, wine.feature_names.index('flavanoids')]\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Instantiate and fit your decision tree for regression\n",
    "tree_regressor = TreeNode()\n",
    "tree_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on the Wine Quality dataset: {mse:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
